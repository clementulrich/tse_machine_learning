{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "744eea0c",
   "metadata": {},
   "source": [
    "# ðŸ§ª PyTorch Lab 7: Language Modeling\n",
    "\n",
    "\n",
    "This is a very large lab session. We will go over manipulating textual data with deep learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ff44fd",
   "metadata": {},
   "source": [
    "## 0. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b92e2ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from urllib.request import urlopen\n",
    "import nltk\n",
    "\n",
    "#You might need to download nltlk and run these once : \n",
    "#nltk.download('punkt_tab')\n",
    "#nltk.download(\"punkt\")\n",
    "\n",
    "import math\n",
    "import random\n",
    "from typing import List, Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7961c254",
   "metadata": {},
   "source": [
    "## 1. Download a book from projet gutenberg\n",
    "We'll start by dowloading Alice in wonderland from the gutenberg project. Just use my code and check what it outputs. It's quite easy to understand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29c5b772-ed06-46a0-aa6b-cad77180e1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = \"https://www.gutenberg.org/ebooks/11.txt.utf-8\"\n",
    "OUT_SENTENCES = Path(\"Alice_in_Wonderland_sentences.txt\")\n",
    "OUT_WORDS = Path(\"Alice_in_Wonderland_words.txt\")\n",
    "\n",
    "\n",
    "def strip_gutenberg_boilerplate(text: str) -> str:\n",
    "    start_marks = [\n",
    "        \"*** START OF THIS PROJECT GUTENBERG EBOOK\",\n",
    "        \"***START OF THE PROJECT GUTENBERG EBOOK\",\n",
    "        \"*** START OF THE PROJECT GUTENBERG EBOOK\",\n",
    "    ]\n",
    "    end_marks = [\n",
    "        \"*** END OF THIS PROJECT GUTENBERG EBOOK\",\n",
    "        \"***END OF THE PROJECT GUTENBERG EBOOK\",\n",
    "        \"*** END OF THE PROJECT GUTENBERG EBOOK\",\n",
    "    ]\n",
    "\n",
    "    start_idx = 0\n",
    "    for mark in start_marks:\n",
    "        i = text.find(mark)\n",
    "        if i != -1:\n",
    "            start_idx = text.find(\"\\n\", i) + 1\n",
    "            break\n",
    "\n",
    "    end_idx = len(text)\n",
    "    for mark in end_marks:\n",
    "        i = text.find(mark)\n",
    "        if i != -1:\n",
    "            end_idx = text.rfind(\"\\n\", 0, i)\n",
    "            break\n",
    "\n",
    "    return text[start_idx:end_idx].strip()\n",
    "\n",
    "with urlopen(URL) as r:\n",
    "    raw = r.read().decode(\"utf-8\", errors=\"replace\")\n",
    "\n",
    "\n",
    "cleaned = strip_gutenberg_boilerplate(raw)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6561cc5-056c-477c-ab43-1a3bd99d30ef",
   "metadata": {},
   "source": [
    "## 2. Preprocessing the text \n",
    "\n",
    "we'll create functions to preprocess the text and put it in a matrix usnig word indexing\n",
    "Use the two functions given to fill the TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f80caae-8e48-470b-8b5f-37e53ef075dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 1028, 56, 1761, 32, 1040, 2410, 267, 1189, 291, 2113, 1309, 825, 613, 405, 318, 1020, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "<start> illustration alice s adventures in wonderland by lewis carroll the millennium fulcrum edition contents chapter i <stop> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n"
     ]
    }
   ],
   "source": [
    "PAD_ID = 0\n",
    "UNK_ID = 1\n",
    "START_ID = 2\n",
    "STOP_ID = 3\n",
    "\n",
    "def tokenize_words(s: str):\n",
    "    # one place to define your tokenization rules\n",
    "    return [w for w in nltk.word_tokenize(s.lower()) if w.isalpha()]\n",
    "    \n",
    "def text_to_sentences(text: str):\n",
    "    text = \" \".join(text.split())  # normalize whitespace\n",
    "    return nltk.sent_tokenize(text)\n",
    "\n",
    "\n",
    "#Build word index : mapping each word in the vocabulary to an indice eg {'<pad>' : 0, \"<unk>\" : 1, \"<start>\" : 2, \"<stop>\": 3, \"hello\": 4, \"bye\": 5} and its reverse index \n",
    "def build_vocab_from_sentences(sentences):\n",
    "    #TODO\n",
    "\n",
    "#Should output a list of list of size n_sentences, max_lenght, where each row is a sentences, i.e. a sequence of words (replaced by its indice) \n",
    "#Add the start token at begining, stop token at the end, and pad to the longest sentence size, meaning fill with 0 so that each list is the same size\n",
    "def sentences_to_matrix(sentences, word2idx, pad_id=PAD_ID, unk_id=UNK_ID):\n",
    "    #TODO\n",
    "    \n",
    "#This function should take a list of words and output a sequence of indices from the vocabulary index\n",
    "def encode(tokens, word2idx):\n",
    "    #TODO\n",
    "#This function should take a list of words indices and output a sequence of words\n",
    "def decode(ids, idx2word):\n",
    "    #TODO\n",
    "\n",
    "\n",
    "sentences = text_to_sentences(cleaned)\n",
    "\n",
    "\n",
    "word2idx,idx2word = build_vocab_from_sentences(sentences)\n",
    "matrix = sentences_to_matrix(sentences, word2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60b0cd8-5fbc-411b-9e92-f442cb45bdb0",
   "metadata": {},
   "source": [
    "For this code \n",
    "\n",
    "```python\n",
    "print(matrix[0])\n",
    "print(decode(matrix[0], idx2word))\n",
    "```\n",
    "\n",
    "you should get \n",
    "```python\n",
    "[2, 1028, 56, 1761, 32, 1040, 2410, 267, 1189, 291, 2113, 1309, 825, 613, 405, 318, 1020, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "<start> illustration alice s adventures in wonderland by lewis carroll the millennium fulcrum edition contents chapter i <stop> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae509927-02a2-4a94-9354-cdbc77ff1137",
   "metadata": {},
   "source": [
    "Now we put everything in a data loader\n",
    "\n",
    "We aim at doing language modeling, so predicting next token. the target is the input shifted by one position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca87fab-1497-4404-bac7-025c5e0ce9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LMDataset(Dataset):\n",
    "    def __init__(self, sequences):\n",
    "        self.seqs = [torch.tensor(seq, dtype=torch.long) for seq in sequences]\n",
    "        # all rows already padded to same length by sentences_to_matrix\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.seqs)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        s = self.seqs[i]\n",
    "        x = s[:-1]  # [T-1]\n",
    "        y = s[1:]   # next-token targets [T-1]\n",
    "        return x, y\n",
    "\n",
    "train_ds = LMDataset(matrix)\n",
    "\n",
    "def collate(batch):\n",
    "    xs, ys = zip(*batch)\n",
    "    return torch.stack(xs, 0), torch.stack(ys, 0)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True, collate_fn=collate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32652688-c1d4-4a68-b1a2-d4371b05fc67",
   "metadata": {},
   "source": [
    "## 3. Creation of a simple model for text generation\n",
    "\n",
    "We provide the code for training a language model from the preprocessed text with an rnn\n",
    "Run and comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a422312-a38a-4178-a2e6-a449e32095e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRNNLM(nn.Module):\n",
    "    def __init__(self, vocab, emb, hid, layers=1, dropout=0.0, padding_idx=PAD_ID):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab, emb, padding_idx=padding_idx)\n",
    "        self.rnn = nn.RNN(\n",
    "            input_size=emb,\n",
    "            hidden_size=hid,\n",
    "            num_layers=layers,\n",
    "            batch_first=True,\n",
    "            nonlinearity=\"tanh\",\n",
    "            dropout=dropout if layers > 1 else 0.0,\n",
    "        )\n",
    "        self.proj = nn.Linear(hid, vocab)\n",
    "\n",
    "    def forward(self, x, h0: Optional[torch.Tensor] = None):\n",
    "        # x: [B, T]\n",
    "        emb = self.embed(x)                 # [B, T, E]\n",
    "        out, h = self.rnn(emb, h0)          # out: [B, T, H]\n",
    "        logits = self.proj(out)             # [B, T, V]\n",
    "        return logits, h\n",
    "\n",
    "vocab_size = max(idx2word.keys()) + 1  # idx2word is {id: token}\n",
    "embedding_dim = 128\n",
    "hidden_size = 256\n",
    "num_layers = 1\n",
    "dropout = 0.1\n",
    "lr = 2e-3\n",
    "epochs = 10\n",
    "\n",
    "model = SimpleRNNLM(vocab_size, embedding_dim, hidden_size, num_layers, dropout).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_ID)  # don't learn on PAD targets\n",
    "optim = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "# --- Train ---\n",
    "for epoch in range(1, epochs + 1):\n",
    "    model.train()\n",
    "    running = 0.0\n",
    "    steps = 0\n",
    "    for x, y in train_loader:\n",
    "        x = x.to(device); y = y.to(device)\n",
    "        logits, _ = model(x)  # [B, T, V]\n",
    "        loss = criterion(logits.reshape(-1, vocab_size), y.reshape(-1))\n",
    "        optim.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optim.step()\n",
    "        running += loss.item()\n",
    "        steps += 1\n",
    "    print(f\"epoch {epoch}/{epochs} - loss {running/steps:.4f} \")\n",
    "\n",
    "# --- Autoregressive generation ---\n",
    "@torch.no_grad()\n",
    "def sample(\n",
    "    prompt: str = \"\",\n",
    "    max_new_tokens: int = 50,\n",
    "    temperature: float = 1.0,\n",
    "    top_k: Optional[int] = 50,\n",
    "    stop_id: int = STOP_ID,\n",
    "):\n",
    "\n",
    "    model.eval()\n",
    "    # Build initial context\n",
    "    if prompt.strip():\n",
    "        init_tokens = [START_ID] + encode(tokenize_words(prompt), word2idx)\n",
    "    else:\n",
    "        init_tokens = [START_ID]\n",
    "\n",
    "    x = torch.tensor([init_tokens], dtype=torch.long, device=device)  # [1, T]\n",
    "    # Prime the hidden state by running the context through the RNN\n",
    "    logits, h = model(x)\n",
    "    next_id = int(torch.argmax(logits[0, -1]).item())\n",
    "\n",
    "    generated: List[int] = init_tokens.copy()\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        # Feed only last token each step (true autoregressive)\n",
    "        last = torch.tensor([[generated[-1]]], dtype=torch.long, device=device)  # [1,1]\n",
    "        out, h = model.embed(last), h  # reuse hidden state\n",
    "        out, h = model.rnn(out, h)     # [1,1,H], h updated\n",
    "        logits = model.proj(out[:, -1, :])  # [1,V]\n",
    "        logits = logits / max(temperature, 1e-6)\n",
    "\n",
    "        # Optional top-k filtering\n",
    "        if top_k is not None and top_k > 0:\n",
    "            top_vals, top_idx = torch.topk(logits, k=min(top_k, logits.size(-1)), dim=-1)\n",
    "            probs = torch.softmax(top_vals, dim=-1)\n",
    "            next_token = top_idx.gather(-1, torch.multinomial(probs, num_samples=1))\n",
    "        else:\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "        next_id = int(next_token.item())\n",
    "        generated.append(next_id)\n",
    "        if next_id == stop_id:\n",
    "            break\n",
    "\n",
    "    # Convert to text, skipping specials\n",
    "    specials = {PAD_ID, UNK_ID, START_ID, STOP_ID}\n",
    "    words = [idx2word.get(i, \"<unk>\") for i in generated if i not in specials]\n",
    "    return \" \".join(words)\n",
    "\n",
    "print(\"\\n=== Samples ===\")\n",
    "print(sample(\"Once upon a time\", max_new_tokens=30, temperature=0.8))\n",
    "print(sample(\"\", max_new_tokens=30, temperature=1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf07742-6f84-4b26-8028-e9495eb3e7f9",
   "metadata": {},
   "source": [
    "## 4. Implementing a single layer single head Transformer\n",
    "\n",
    "We will now create a single head single layer Transformer. Enjoy ^^"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f3bad3-6664-418e-a435-d568f798bb94",
   "metadata": {},
   "source": [
    "First, create the function to build a positional encoding matrix of size $n$ (e.g 512, corresponds to the max length of the senten es) and for a given embedding dimension $d_{emb}$\n",
    "\n",
    "Remember the equation for the positionnal encoding from Vaswani Paper is \n",
    "\n",
    "For each position $k$ and dimension $i$:\n",
    "\n",
    "$$\n",
    "PE_{(k, 2i)} = \\sin\\left(\\frac{k}{n^{2i/d_{emb}}}\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "PE_{(k, 2i+1)} = \\cos\\left(\\frac{k}{n^{2i/d_{emb}}}\\right)\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $k$ is the position index in the sequence, from 0 to $n-1$\n",
    "- $i$ is the dimension index\n",
    "- $d_{\\text{model}}$ is the dimensionality of the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0cd663f-ed24-4469-9885-c4c0bbe648f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_positional(max_L,emb):\n",
    "        posit = torch.zeros(max_L,emb)\n",
    "        #TODO\n",
    "        return posit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57fb4d6a-7dd7-4a53-aa65-be372bcb18b2",
   "metadata": {},
   "source": [
    "Plot the image "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20c9514-edeb-4cb3-bf86-40eb004de72b",
   "metadata": {},
   "source": [
    "We will implement the Transformer for now. We provide the element for the architecture. Justy implement the forward function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5bf1055-9d90-420f-b3ae-c729d37b463a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTransformer(nn.Module):\n",
    "    def __init__(self, max_L, vocab, emb, padding_idx=PAD_ID):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab, emb, padding_idx=padding_idx)\n",
    "        self.util_emb = math.pow(emb,1/2)\n",
    "        self.positional = create_positional(max_L,emb)\n",
    "        self.key = torch.nn.Linear(emb,emb)\n",
    "        self.query = torch.nn.Linear(emb,emb)\n",
    "        self.value = torch.nn.Linear(emb,emb)\n",
    "        self.proj = torch.nn.Linear(emb,emb)\n",
    "\n",
    "        self.out = nn.Linear(emb, vocab, bias=False)\n",
    "        self.out.weight = self.embed.weight\n",
    "        \n",
    "    \n",
    "        \n",
    "    def forward(self, x, h0: Optional[torch.Tensor] = None):\n",
    "        # embed x\n",
    "        \n",
    "        #Add the positionnal encoding\n",
    "\n",
    "        #Build the key value and query by projecting x\n",
    "\n",
    "        #Compute attention\n",
    "        #Put the upper triangle to -inf before passing it to softmax in order to only attend attention to the left part of the sentence\n",
    "        #pass attention to softmax\n",
    "\n",
    "        #project\n",
    "\n",
    "        #add to value of x before attention\n",
    "\n",
    "        #pass into the out layer to output the logits over the vocabulary\n",
    "    \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34679e7-061f-45b1-aeaf-a9aeb3a49e7e",
   "metadata": {},
   "source": [
    "Now use this code to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72e26627-50ec-4872-8f8d-f3d690f5fa12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1/20 - loss 100.9125 - approx ppl 0.00\n",
      "epoch 2/20 - loss 70.3420 - approx ppl 0.00\n",
      "epoch 3/20 - loss 54.3770 - approx ppl 0.00\n",
      "epoch 4/20 - loss 43.4236 - approx ppl 0.00\n",
      "epoch 5/20 - loss 35.3034 - approx ppl 0.00\n",
      "epoch 6/20 - loss 28.9026 - approx ppl 0.00\n",
      "epoch 7/20 - loss 23.5463 - approx ppl 0.00\n",
      "epoch 8/20 - loss 19.0944 - approx ppl 0.00\n",
      "epoch 9/20 - loss 15.1518 - approx ppl 0.00\n",
      "epoch 10/20 - loss 11.6649 - approx ppl 0.00\n",
      "epoch 11/20 - loss 9.4477 - approx ppl 0.00\n",
      "epoch 12/20 - loss 8.0856 - approx ppl 0.00\n",
      "epoch 13/20 - loss 7.2034 - approx ppl 0.00\n",
      "epoch 14/20 - loss 6.6410 - approx ppl 0.00\n",
      "epoch 15/20 - loss 6.2455 - approx ppl 0.00\n",
      "epoch 16/20 - loss 5.9750 - approx ppl 0.00\n",
      "epoch 17/20 - loss 5.7685 - approx ppl 0.00\n",
      "epoch 18/20 - loss 5.6028 - approx ppl 0.00\n",
      "epoch 19/20 - loss 5.4685 - approx ppl 0.00\n",
      "epoch 20/20 - loss 5.3578 - approx ppl 0.00\n"
     ]
    }
   ],
   "source": [
    "vocab_size = max(idx2word.keys()) + 1  # idx2word is {id: token}\n",
    "embedding_dim = 128\n",
    "lr = 2e-3\n",
    "epochs = 20\n",
    "max_Length = 512\n",
    "\n",
    "model = SimpleTransformer(max_Length, vocab_size, embedding_dim).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_ID)  # don't learn on PAD targets\n",
    "optim = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    model.train()\n",
    "    running = 0.0\n",
    "    steps = 0\n",
    "    for x, y in train_loader:\n",
    "        x = x.to(device); y = y.to(device)\n",
    "        logits = model(x)  # [B, T, V]\n",
    "        loss = criterion(logits.reshape(-1, vocab_size), y.reshape(-1))\n",
    "        optim.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optim.step()\n",
    "        running += loss.item()\n",
    "        steps += 1\n",
    "    print(f\"epoch {epoch}/{epochs} - loss {running/steps:.4f} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93db6a0b-ad8e-4a15-85f3-7756f0466988",
   "metadata": {},
   "source": [
    "We'll create a function to do sampling. Fill the gaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cfee5283-8225-486f-b589-5878ed21e086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Samples ===\n",
      "[[2, 1433, 2274, 4, 2157]]\n",
      "once upon a time the gryphon t said alice t said alice t said alice t a large the gryphon t a large to the mock turtle and and\n",
      "[[2]]\n",
      "i must be a large voice and went on it s and said alice t said alice t a large the gryphon t know a very much\n"
     ]
    }
   ],
   "source": [
    "# --- Autoregressive generation ---\n",
    "@torch.no_grad()\n",
    "def sample(\n",
    "    prompt: str = \"\",\n",
    "    max_new_tokens: int = 50,\n",
    "    temperature: float = 1.0,\n",
    "    top_k: Optional[int] = 50,\n",
    "    stop_id: int = STOP_ID,\n",
    "):\n",
    "\n",
    "    model.eval()\n",
    "    # Build initial context\n",
    "    if prompt.strip():\n",
    "        init_tokens = [START_ID] + encode(tokenize_words(prompt), word2idx)\n",
    "    else:\n",
    "        init_tokens = [START_ID]\n",
    "    seq = [init_tokens]\n",
    "    \n",
    "    for _ in range(max_new_tokens):\n",
    "        #TODO\n",
    "        seq = [seq[0] + [next_id]]  \n",
    "        \n",
    "    specials = {PAD_ID, UNK_ID, START_ID, STOP_ID}\n",
    "    words = [idx2word.get(i, \"<unk>\") for i in seq[0] if i not in specials]\n",
    "    \n",
    "    return \" \".join(words)\n",
    "\n",
    "# --- Quick demo ---\n",
    "print(\"\\n=== Samples ===\")\n",
    "print(sample(\"Once upon a time\", max_new_tokens=30, temperature=0.8))\n",
    "print(sample(\"\", max_new_tokens=30, temperature=1.0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
