{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6cae0bb0",
   "metadata": {},
   "source": [
    "# ðŸ§ª PyTorch Lab 2: Newton Method for logistic regression\n",
    "\n",
    "\n",
    "You already know **gradient descent**: update the parameters in the opposite direction of the gradient, with a fixed or adaptive step size.\n",
    "\n",
    "**Newtonâ€™s method** goes a step further. Instead of using only the first derivative (the gradient), it also uses the **second derivative information** (the Hessian matrix). Around the current point, we approximate the loss by a quadratic function, then jump directly to its minimum.  \n",
    "\n",
    "The update rule is:\n",
    "\n",
    "$$\n",
    "w_{k+1} = w_k - H^{-1}(w_k)\\,\\nabla f(w_k)\n",
    "$$\n",
    "\n",
    "where:  \n",
    "- $ \\nabla f(w_k)$ is the gradient at the current point,  \n",
    "- $H(w_k)$ is the Hessian (matrix of second derivatives).  \n",
    "\n",
    "\n",
    "We'll train on a small **real dataset** (Breast Cancer Wisconsin from scikit-learn). If scikit-learn isn't available, we fall back to a tiny embedded Iris-binary subset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1f2927",
   "metadata": {},
   "source": [
    "## 1) Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08889593",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.set_default_dtype(torch.float64)\n",
    "print('PyTorch version:', torch.__version__)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# For data\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e1c2fd",
   "metadata": {},
   "source": [
    "## 2) Load & prepare the data (Iris)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1431a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load iris dataset\n",
    "data = load_iris()\n",
    "X = data['data']          # shape (150, 4)\n",
    "y = data['target']        # classes: 0=setosa, 1=versicolor, 2=virginica\n",
    "\n",
    "# Convert to binary classification: setosa (0) vs not-setosa (1)\n",
    "y = (y != 0).astype(float)\n",
    "\n",
    "# Standardize features\n",
    "X = StandardScaler().fit_transform(X)\n",
    "\n",
    "# Convert to torch tensors\n",
    "X = torch.tensor(X, dtype=torch.get_default_dtype())\n",
    "y = torch.tensor(y, dtype=torch.get_default_dtype())\n",
    "\n",
    "n, d = X.shape\n",
    "\n",
    "# Add bias column (intercept)\n",
    "X = torch.cat([torch.ones(n, 1, dtype=X.dtype), X], dim=1)\n",
    "d = d + 1\n",
    "print(f\"Tensors: X={tuple(X.shape)}, y={tuple(y.shape)}, d={d}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34385f0",
   "metadata": {},
   "source": [
    "## 3) Define the logistic loss\n",
    "\n",
    "For a weight vector \\(w \\in \\mathbb{R}^{d}\\) and data \\(X \\in \\mathbb{R}^{n\\times d}\\), labels \\(y \\in \\{0,1\\}^n\\):\n",
    "\n",
    "$$\n",
    "\\ell(w) = -\\frac{1}{n}\\sum_{i=1}^n \\Big( y_i\\log(\\sigma(x_i^\\top w)) + (1-y_i)\\log(1-\\sigma(x_i^\\top w)) \\Big),\n",
    "$$\n",
    "\n",
    "where $\\sigma(t) = 1/(1+e^{-t})$ is the sigmoid.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da9a1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_loss(w, X, y):\n",
    "    z = X @ w\n",
    "    p = torch.sigmoid(z)\n",
    "    eps = 1e-12\n",
    "    nll = - (y * torch.log(p + eps) + (1 - y) * torch.log(1 - p + eps)).mean()\n",
    "    return nll\n",
    "\n",
    "# Initialize parameters\n",
    "w = torch.nn.Parameter(torch.rand(X.shape[1], dtype=torch.get_default_dtype()), requires_grad=True)\n",
    "\n",
    "# Quick sanity check\n",
    "loss = logistic_loss(w, X, y)\n",
    "print(\"Initial loss:\", loss.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ccb34fe",
   "metadata": {},
   "source": [
    "## 4) **Exercise:** Gradient & Hessian with `torch.autograd.functional`\n",
    "\n",
    "**Tasks**\n",
    "1. Compute the gradient of `logistic_loss` at `w`.\n",
    "2. Compute the Hessian at `w`.\n",
    "   \n",
    "hint : torch.autograd.functional \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65474d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction de perte logistique existante\n",
    "def logistic_loss(w, X, y):\n",
    "    z = X @ w\n",
    "    p = torch.sigmoid(z)\n",
    "    eps = 1e-12\n",
    "    nll = -(y * torch.log(p + eps) + (1 - y) * torch.log(1 - p + eps)).mean()\n",
    "    return nll\n",
    "\n",
    "grad = torch.autograd.functional.grad(lambda w_: logistic_loss(w_, X, y), w)\n",
    "H = torch.autograd.functional.hessian(lambda w_: logistic_loss(w_, X, y), w)\n",
    "\n",
    "print(\"||grad||:\", float(torch.norm(grad)))\n",
    "print(\"H shape:\", tuple(H.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19db0c54",
   "metadata": {},
   "source": [
    "## 5) Gradient Descent (GD)\n",
    "\n",
    "We implement a basic GD loop:\n",
    "$$\n",
    "w \\leftarrow w - \\alpha \\nabla \\ell(w).\n",
    "$$\n",
    "\n",
    "**Exercise**\n",
    "- Complete the TODOs to record the loss at each epoch and update $w$.\n",
    "- Use a stopping criterion based on $$ \\|\\nabla \\ell(w)\\| < \\varepsilon $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e7ac57",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epoch = 100\n",
    "alpha = 1.0       # taux d'apprentissage\n",
    "eps = 1e-2        # seuil de convergence pour la norme du gradient\n",
    "loss_grad = []\n",
    "\n",
    "# RÃ©initialisation des poids\n",
    "w = torch.nn.Parameter(torch.rand(X.shape[1], dtype=torch.get_default_dtype()), requires_grad=True)\n",
    "\n",
    "for epoch in range(n_epoch):\n",
    "    # Calcul de la perte\n",
    "    loss = logistic_loss(w, X, y)\n",
    "    loss_grad.append(float(loss))\n",
    "    \n",
    "    # Calcul du gradient\n",
    "    grad = torch.autograd.functional.grad(lambda w_: logistic_loss(w_, X, y), w)[0]\n",
    "    \n",
    "    # Mise Ã  jour du vecteur poids w selon la rÃ¨gle GD\n",
    "    with torch.no_grad():\n",
    "        w -= alpha * grad\n",
    "    \n",
    "    # CritÃ¨re d'arrÃªt basÃ© sur la norme du gradient\n",
    "    if torch.norm(grad) < eps:\n",
    "        print(f\"GD early stop at epoch {epoch}\")\n",
    "        break\n",
    "\n",
    "print(\"Final loss:\", loss_grad[-1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac85ad3b",
   "metadata": {},
   "source": [
    "## 6) Newton's Method\n",
    "\n",
    "Newton update:\n",
    "$$\n",
    "w \\leftarrow w - H(w)^{-1} \\nabla \\ell(w).\n",
    "$$\n",
    "\n",
    "In practice, **solve** the linear system `H s = grad` and set `w â† w âˆ’ s` to avoid explicit inversion.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ab96ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epoch = 100\n",
    "eps = 1e-2\n",
    "loss_newton = []\n",
    "\n",
    "# RÃ©initialiser w\n",
    "w = torch.nn.Parameter(torch.rand(X.shape[1], dtype=torch.get_default_dtype()), requires_grad=True)\n",
    "\n",
    "for epoch in range(n_epoch):\n",
    "    # Calculer la perte\n",
    "    loss = logistic_loss(w, X, y)\n",
    "    loss_newton.append(float(loss))\n",
    "    \n",
    "    # Calculer gradient et Hessienne avec la fonction lambda\n",
    "    grad = torch.autograd.functional.grad(lambda w_: logistic_loss(w_, X, y), w)[0]\n",
    "    H = torch.autograd.functional.hessian(lambda w_: logistic_loss(w_, X, y), w)\n",
    "    \n",
    "    # RÃ©soudre H * step = grad pour obtenir le pas de Newton\n",
    "    step = torch.linalg.solve(H, grad)\n",
    "    \n",
    "    # Mise Ã  jour de w avec le pas de Newton (sans trace dans autograd)\n",
    "    with torch.no_grad():\n",
    "        w -= step\n",
    "    \n",
    "    # CritÃ¨re d'arrÃªt\n",
    "    if torch.norm(grad) < eps:\n",
    "        print(f\"Newton early stop at epoch {epoch}\")\n",
    "        break\n",
    "\n",
    "print(\"Final loss:\", loss_newton[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299f2abb",
   "metadata": {},
   "source": [
    "## 7) Compare convergence\n",
    "\n",
    "Plot the loss trajectories of GD vs Newton.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a358a33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Supposons loss_grad et loss_newton contiennent les historiques de pertes des deux mÃ©thodes\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(loss_grad, label='Gradient Descent')\n",
    "plt.plot(loss_newton, label='Newton Method')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Comparison of Loss Convergence')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
